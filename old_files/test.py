import time
from langchain_ollama import ChatOllama

# --- AYARLAR ---
MODEL_NAME = "qwen2.5-coder:7b"
# 8GB VRAM iÃ§in gÃ¼venli context limiti. 
# Bunu ileride ihtiyaca gÃ¶re artÄ±racaÄŸÄ±z ama baÅŸlangÄ±Ã§ iÃ§in 4096 ideal.
CONTEXT_WINDOW = 4096 

def test_connection():
    print(f"ğŸ”„ Model yÃ¼kleniyor: {MODEL_NAME}...")
    print(f"ğŸ’¾ Hedeflenen Context Penceresi: {CONTEXT_WINDOW} token")
    
    try:
        # LLM TanÄ±mlama
        llm = ChatOllama(
            model=MODEL_NAME,
            temperature=0.1, # Daha tutarlÄ± kod/mantÄ±k iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
            num_ctx=CONTEXT_WINDOW
        )

        start_time = time.time()
        
        # Basit bir mantÄ±k sorusu soralÄ±m
        query = "Bir Python listesindeki tekrar eden elemanlarÄ± silmenin en performanslÄ± yolu nedir? Tek cÃ¼mleyle aÃ§Ä±kla."
        print(f"\nâ“ Soru: {query}")
        
        response = llm.invoke(query)
        
        end_time = time.time()
        duration = end_time - start_time

        print(f"\nâœ… CEVAP:\n{response.content}")
        print(f"\nâ±ï¸ GeÃ§en SÃ¼re: {duration:.2f} saniye")
        print("ğŸ‰ Kurulum BaÅŸarÄ±lÄ±! Sistem 8GB VRAM Ã¼zerinde Ã§alÄ±ÅŸmaya hazÄ±r.")

    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        print("LÃ¼tfen 'ollama serve' komutunun Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan veya model isminin doÄŸru olduÄŸundan emin ol.")

if __name__ == "__main__":
    test_connection()